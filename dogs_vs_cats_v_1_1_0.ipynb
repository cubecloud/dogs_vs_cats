{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dogs_vs_cats-v-1-1-0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cubecloud/dogs_vs_cats/blob/master/dogs_vs_cats_v_1_1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAl-JTvzvbHC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qJfJPd0PrzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "%tensorflow_version 1.15.2\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NJL_gODxWhp",
        "colab_type": "code",
        "outputId": "b852b5a2-12bc-4c3d-ff79-0d30792d3376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "%tensorflow_version 1.15.2\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.15.2`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TTUHDm3P7oZ",
        "colab_type": "code",
        "outputId": "3b2ae576-a3a1-4f1c-da1a-9071dfbdfe7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D-uEuguvcgl",
        "colab_type": "code",
        "outputId": "9c23be8c-ca57-4db8-fcf8-52aacf3d787e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "%tensorflow_version 1.15.2\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "import os\n",
        "import glob\n",
        "import pytz\n",
        "import datetime\n",
        "from math import exp\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.15.2`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR_nxwysvW3p",
        "colab_type": "code",
        "outputId": "37d9bcd9-eccb-4595-d870-44c080e8443a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "version=110\n",
        "timezone = pytz.timezone(\"Europe/Moscow\")\n",
        "image_size = 150\n",
        "batch_size = 40\n",
        "epochs = 150\n",
        "start_learning_rate = 0.001\n",
        "start_patience = round(epochs*0.04)\n",
        "\n",
        "print (f'Image Size = {image_size}x{image_size}')\n",
        "\n",
        "def make_model(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    # Image augmentation block\n",
        "    # x = data_augmentation(inputs)\n",
        "    x = inputs\n",
        "    # Entry block\n",
        "    # x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(x)\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization( momentum=0.99 )(x) # added momentum [ 0.9 ]\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    x = layers.Dropout(0.7)(x) #<= added dropout layer [ 0.5 ]\n",
        "\n",
        "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization( momentum=0.9 )(x) # added momentum [ 0.9 ]\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    for size in [128, 256, 512, 728]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        \n",
        "        # x = layers.Dropout(0.2)(x) # <= added dropout layer [ ]\n",
        "\n",
        "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization( momentum=0.9)(x) # added momentum [ 0.9 ]\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        \n",
        "        # x = layers.Dropout(0.2)(x) #<= added dropout layer [ ]\n",
        "        \n",
        "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization( momentum=0.9 )(x) # added momentum [ 0.9 ]\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization( momentum=0.9 )(x) # added momentum [ 0.9 ]\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # x = layers.Dropout(0.5)(x) #<= added dropout layer [ 0.2 0.5 0.5 ]\n",
        "    \n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    if num_classes == 2:\n",
        "        activation = \"sigmoid\"\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = \"softmax\"\n",
        "        units = num_classes\n",
        "\n",
        "    x = layers.Dropout(0.6)(x) #<= dropout [ 0.5 0.52 0.5 0.6 0.6  ]\n",
        "    outputs = layers.Dense(units, activation=activation)(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "home_dir = \"/content/drive/My Drive/Colab Notebooks/dogs_vs_cats/\"\n",
        "base_dir = \"/content/drive/My Drive/Colab Notebooks/dogs_vs_cats/data/\"\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# data loading\n",
        "# ----------------------------------------------------\n",
        "\n",
        "x_train = np.load(os.path.join(home_dir, 'dogs_vs_cats_photos_train.npy'))\n",
        "y_train = np.load(os.path.join(home_dir,'dogs_vs_cats_labels_train.npy'))[2:,]\n",
        "x_validation = np.load(os.path.join(home_dir,'dogs_vs_cats_photos_validation.npy'))\n",
        "y_validation = np.load(os.path.join(home_dir,'dogs_vs_cats_labels_validation.npy'))[2:,]\n",
        "print (x_train.shape, y_train.shape)\n",
        "print (x_validation.shape, y_validation.shape)\n",
        "# ----------------------------------------------------\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30, # <= [ 40, 30 ]\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False,\n",
        "    fill_mode=\"nearest\")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_datagen.fit(x_train)\n",
        "\n",
        "train_generator = train_datagen.flow(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "validation_generator = validation_datagen.flow(\n",
        "    x=x_validation,\n",
        "    y=y_validation,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "model = make_model(input_shape=(image_size,image_size) + (3,), num_classes=2)\n",
        "keras.utils.plot_model(model, show_shapes=True)\n",
        "model.summary()\n",
        "\n",
        "def scheduler(epoch):\n",
        "  if epoch < 10:\n",
        "    return start_learning_rate\n",
        "  else:\n",
        "    # return start_learning_rate * exp(0.1 * (10 - epoch))\n",
        "    return start_learning_rate*(0.75**(epoch//10))\n",
        "\n",
        "class EarlyStoppingAtMinValLoss(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, start_learning_rate=start_learning_rate, patience=3):\n",
        "    super(EarlyStoppingAtMinValLoss, self).__init__()\n",
        "    self.patience = patience\n",
        "    self.start_learning_rate = start_learning_rate\n",
        "    # best_weights to store the weights at which the minimum loss occurs.\n",
        "    self.best_weights = None\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    # The number of epoch it has waited when loss is no longer minimum.\n",
        "    self.wait = 0\n",
        "    # The epoch the training stops at.\n",
        "    self.stopped_epoch = 0\n",
        "    # Initialize the best as infinity.\n",
        "    self.best = np.Inf\n",
        "  \n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    if (epoch >= 10) and (epoch%10 == 0):\n",
        "      self.patience += round((epochs-epoch)*0.03)\n",
        "    # self.patience = round((self.start_learning_rate / scheduler (epoch) + (epochs*0.15))/2)\n",
        "    print (f'Patience: {self.patience}')\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    current = logs.get('val_loss')\n",
        "    if np.less(current, self.best):\n",
        "      self.best = current\n",
        "      self.wait = 0\n",
        "      # Record the best weights if current results is better (less).\n",
        "      self.best_weights = self.model.get_weights()\n",
        "    else:\n",
        "      self.wait += 1\n",
        "      if self.wait >= self.patience:\n",
        "        self.stopped_epoch = epoch\n",
        "        self.model.stop_training = True\n",
        "        print('Restoring model weights from the end of the best epoch.')\n",
        "        self.model.set_weights(self.best_weights)\n",
        "\n",
        "  def on_train_end(self, logs=None):\n",
        "    if self.stopped_epoch > 0:\n",
        "      print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
        "\n",
        "  # def on_train_batch_begin(self, batch, logs=None):\n",
        "  #   print('Training: batch {} begins at {}'.format(batch, datetime.datetime.now(timezone).time()))\n",
        "\n",
        "\n",
        "def filename_with_time (prefix_str, root_str=''):\n",
        "  return prefix_str + root_str + str(datetime.datetime.now(timezone).strftime(\"%d-%m-%Y_%H-%M\"))\n",
        "\n",
        "lrs = keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
        "log_dir = os.path.join(home_dir, 'logs/fit')\n",
        "esmvl = EarlyStoppingAtMinValLoss (patience=start_patience)\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=start_patience, verbose=1)\n",
        "tb = TensorBoard(log_dir=log_dir, histogram_freq=10, write_graph=True, write_images=False )\n",
        "csv = CSVLogger(os.path.join(home_dir,'save/', filename_with_time(model_name) + \".log\"))\n",
        "\n",
        "model_name= f'cats_and_dogs_{str(version)}_{str(image_size)}x{str(image_size)}_{str(batch_size)}_'\n",
        "chkp = keras.callbacks.ModelCheckpoint(os.path.join(home_dir,'save/', filename_with_time(model_name,\"save_at_{epoch:02d}-{val_loss:.4f}_\") + \".h5\"), monitor='val_loss', save_best_only=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(lr=start_learning_rate),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "callbacks = [lrs,  esmvl, chkp, csv ]\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = len(train_generator),\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = len(validation_generator),\n",
        "    epochs = epochs,\n",
        "    verbose=1,\n",
        "    callbacks = callbacks)\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print (f'Planned epochs: {epochs} Calculated epochs : {len(history.history[\"loss\"])} Time elapsed: {end - start}')\n",
        "epochs = len(history.history['loss'])\n",
        "model_name= f'cats_and_dogs_{str(version)}_{str(image_size)}x{str(image_size)}_{str(batch_size)}_epochs-{epochs}_'\n",
        "model.save(os.path.join(home_dir, model_name + str(datetime.datetime.now(timezone).strftime(\"%d-%m-%Y_%H-%M\")) + '.h5'))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(15,12))\n",
        "gs = fig.add_gridspec(1, 1)\n",
        "ax1 = fig.add_subplot()\n",
        "# Don't allow the axis to be on top of your data\n",
        "ax1.set_axisbelow(True)\n",
        "# Turn on the minor TICKS, which are required for the minor GRID\n",
        "ax1.minorticks_on()\n",
        "# Customize the major grid\n",
        "ax1.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
        "# Customize the minor grid\n",
        "ax1.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\n",
        "N = np.arange(0, len(history.history[\"acc\"]) )\n",
        "plt.plot(N, history.history[\"loss\"], linestyle='-', color='red', label=\"Training loss\")\n",
        "plt.plot(N, history.history[\"acc\"], linestyle='-.', color='magenta', label=\"Training accuracy\")\n",
        "plt.plot(N, history.history[\"val_loss\"], linestyle='--', color='blue', label=\"Validation loss\")\n",
        "plt.plot(N, history.history[\"val_acc\"], linestyle=':', color='violet', label=\"Validation accuracy\")\n",
        "plt.title(\"Training/validation Loss and Accuracy\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(home_dir,'figures', ('fig_' + model_name + str(datetime.datetime.now(timezone).strftime(\"%d-%m-%Y_%H-%M\"))+'.png')))\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image Size = 150x150\n",
            "(18800, 150, 150, 3) (18800,)\n",
            "(6200, 150, 150, 3) (6200,)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 150, 150, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 75, 75, 32)   896         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 75, 75, 32)   128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 75, 75, 32)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 75, 75, 32)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 75, 75, 64)   18496       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 75, 75, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 75, 75, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 75, 75, 64)   0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d (SeparableConv (None, 75, 75, 128)  8896        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 75, 75, 128)  512         separable_conv2d[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 75, 75, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 75, 75, 128)  17664       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 75, 75, 128)  512         separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 38, 38, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 38, 38, 128)  8320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 38, 38, 128)  0           max_pooling2d[0][0]              \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 38, 38, 128)  0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 38, 38, 256)  34176       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 38, 38, 256)  1024        separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 38, 38, 256)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_3 (SeparableCo (None, 38, 38, 256)  68096       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 38, 38, 256)  1024        separable_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 19, 19, 256)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 19, 19, 256)  33024       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 19, 19, 256)  0           max_pooling2d_1[0][0]            \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 19, 19, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_4 (SeparableCo (None, 19, 19, 512)  133888      activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 19, 19, 512)  2048        separable_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 19, 19, 512)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_5 (SeparableCo (None, 19, 19, 512)  267264      activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 19, 19, 512)  2048        separable_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 512)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 10, 10, 512)  131584      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 10, 10, 512)  0           max_pooling2d_2[0][0]            \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 10, 10, 512)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_6 (SeparableCo (None, 10, 10, 728)  378072      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 10, 10, 728)  2912        separable_conv2d_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 10, 10, 728)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_7 (SeparableCo (None, 10, 10, 728)  537264      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 10, 10, 728)  2912        separable_conv2d_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 728)    0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 5, 5, 728)    373464      add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 5, 5, 728)    0           max_pooling2d_3[0][0]            \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_8 (SeparableCo (None, 5, 5, 1024)   753048      add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 5, 5, 1024)   4096        separable_conv2d_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 5, 5, 1024)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 1024)         0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1024)         0           global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            1025        dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,782,649\n",
            "Trainable params: 2,773,913\n",
            "Non-trainable params: 8,736\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 1/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.6860 - acc: 0.6030Epoch 1/150\n",
            "470/470 [==============================] - 249s 529ms/step - loss: 0.6862 - acc: 0.6028 - val_loss: 0.7918 - val_acc: 0.5358\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 2/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.5836 - acc: 0.6972Epoch 1/150\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.5832 - acc: 0.6976 - val_loss: 0.5134 - val_acc: 0.7411\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 3/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.5236 - acc: 0.7433Epoch 1/150\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.5236 - acc: 0.7434 - val_loss: 0.4977 - val_acc: 0.7605\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 4/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.4818 - acc: 0.7685Epoch 1/150\n",
            "470/470 [==============================] - 232s 495ms/step - loss: 0.4816 - acc: 0.7686 - val_loss: 0.4309 - val_acc: 0.7969\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 5/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.4407 - acc: 0.7948Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.4406 - acc: 0.7949 - val_loss: 0.4436 - val_acc: 0.8055\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 6/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.3997 - acc: 0.8184Epoch 1/150\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.3999 - acc: 0.8182 - val_loss: 0.4397 - val_acc: 0.8032\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 7/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.8377Epoch 1/150\n",
            "470/470 [==============================] - 233s 496ms/step - loss: 0.3632 - acc: 0.8380 - val_loss: 0.3377 - val_acc: 0.8508\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 8/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.3377 - acc: 0.8506Epoch 1/150\n",
            "470/470 [==============================] - 233s 496ms/step - loss: 0.3377 - acc: 0.8504 - val_loss: 0.2852 - val_acc: 0.8861\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 9/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8637Epoch 1/150\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.3145 - acc: 0.8636 - val_loss: 1.0256 - val_acc: 0.6894\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 10/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.8715Epoch 1/150\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.2934 - acc: 0.8714 - val_loss: 0.2091 - val_acc: 0.9166\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 11/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.8826Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.2710 - acc: 0.8826 - val_loss: 0.2239 - val_acc: 0.9085\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 12/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.8929Epoch 1/150\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.2517 - acc: 0.8928 - val_loss: 0.1897 - val_acc: 0.9203\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 13/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.8961Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.2431 - acc: 0.8960 - val_loss: 0.2409 - val_acc: 0.9050\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 14/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9033Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.2287 - acc: 0.9033 - val_loss: 0.2325 - val_acc: 0.9039\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 15/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9052Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.2212 - acc: 0.9052 - val_loss: 0.2911 - val_acc: 0.8781\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 16/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9046Epoch 1/150\n",
            "470/470 [==============================] - 233s 496ms/step - loss: 0.2230 - acc: 0.9047 - val_loss: 0.1705 - val_acc: 0.9289\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 17/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2103 - acc: 0.9128Epoch 1/150\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.2105 - acc: 0.9127 - val_loss: 0.2228 - val_acc: 0.9094\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 18/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9120Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.2071 - acc: 0.9119 - val_loss: 0.1883 - val_acc: 0.9229\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 19/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9147Epoch 1/150\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.2024 - acc: 0.9145 - val_loss: 0.1665 - val_acc: 0.9355\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 20/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9189Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1899 - acc: 0.9190 - val_loss: 0.1701 - val_acc: 0.9350\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 21/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9243Epoch 1/150\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.1809 - acc: 0.9245 - val_loss: 0.1574 - val_acc: 0.9361\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 22/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9300Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1672 - acc: 0.9301 - val_loss: 0.1621 - val_acc: 0.9400\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 23/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9294Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1645 - acc: 0.9294 - val_loss: 0.1616 - val_acc: 0.9376\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 24/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9307Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1659 - acc: 0.9306 - val_loss: 0.1557 - val_acc: 0.9413\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 25/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9336Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1610 - acc: 0.9338 - val_loss: 0.1386 - val_acc: 0.9432\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 26/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9342Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1586 - acc: 0.9343 - val_loss: 0.1388 - val_acc: 0.9442\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 27/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9379Epoch 1/150\n",
            "470/470 [==============================] - 232s 493ms/step - loss: 0.1547 - acc: 0.9378 - val_loss: 0.1701 - val_acc: 0.9315\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 28/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1528 - acc: 0.9376Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1527 - acc: 0.9376 - val_loss: 0.1464 - val_acc: 0.9444\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 29/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9367Epoch 1/150\n",
            "470/470 [==============================] - 232s 493ms/step - loss: 0.1517 - acc: 0.9368 - val_loss: 0.1725 - val_acc: 0.9290\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 30/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9423Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1414 - acc: 0.9423 - val_loss: 0.1454 - val_acc: 0.9466\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 31/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9428Epoch 1/150\n",
            "470/470 [==============================] - 231s 492ms/step - loss: 0.1347 - acc: 0.9428 - val_loss: 0.1463 - val_acc: 0.9442\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 32/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9461Epoch 1/150\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.1332 - acc: 0.9460 - val_loss: 0.1239 - val_acc: 0.9503\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 33/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9506Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1242 - acc: 0.9504 - val_loss: 0.1119 - val_acc: 0.9579\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 34/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9489Epoch 1/150\n",
            "470/470 [==============================] - 232s 493ms/step - loss: 0.1286 - acc: 0.9488 - val_loss: 0.1277 - val_acc: 0.9518\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 35/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9486Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1272 - acc: 0.9486 - val_loss: 0.1745 - val_acc: 0.9344\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 36/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9501Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1237 - acc: 0.9500 - val_loss: 0.1166 - val_acc: 0.9539\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 37/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9501Epoch 1/150\n",
            "470/470 [==============================] - 232s 495ms/step - loss: 0.1220 - acc: 0.9500 - val_loss: 0.1023 - val_acc: 0.9616\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 38/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9546Epoch 1/150\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.1172 - acc: 0.9547 - val_loss: 0.1780 - val_acc: 0.9331\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 39/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9515Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1179 - acc: 0.9516 - val_loss: 0.1368 - val_acc: 0.9455\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 40/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9520Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1199 - acc: 0.9520 - val_loss: 0.1185 - val_acc: 0.9545\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 41/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9585Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1054 - acc: 0.9586 - val_loss: 0.1487 - val_acc: 0.9481\n",
            "\n",
            "Epoch 00042: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 42/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9574Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1058 - acc: 0.9574 - val_loss: 0.1727 - val_acc: 0.9350\n",
            "\n",
            "Epoch 00043: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 43/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9584Epoch 1/150\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.1006 - acc: 0.9584 - val_loss: 0.0998 - val_acc: 0.9598\n",
            "\n",
            "Epoch 00044: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 44/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9598Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.1001 - acc: 0.9598 - val_loss: 0.1173 - val_acc: 0.9552\n",
            "\n",
            "Epoch 00045: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 45/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9586Epoch 1/150\n",
            "470/470 [==============================] - 232s 493ms/step - loss: 0.1026 - acc: 0.9585 - val_loss: 0.1260 - val_acc: 0.9558\n",
            "\n",
            "Epoch 00046: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 46/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9610Epoch 1/150\n",
            "470/470 [==============================] - 232s 493ms/step - loss: 0.0959 - acc: 0.9610 - val_loss: 0.1020 - val_acc: 0.9624\n",
            "\n",
            "Epoch 00047: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 47/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9609Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0966 - acc: 0.9609 - val_loss: 0.1150 - val_acc: 0.9535\n",
            "\n",
            "Epoch 00048: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 48/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9654Epoch 1/150\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.0908 - acc: 0.9655 - val_loss: 0.0988 - val_acc: 0.9627\n",
            "\n",
            "Epoch 00049: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 49/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9639Epoch 1/150\n",
            "470/470 [==============================] - 232s 495ms/step - loss: 0.0881 - acc: 0.9638 - val_loss: 0.1093 - val_acc: 0.9608\n",
            "\n",
            "Epoch 00050: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 50/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9621Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0928 - acc: 0.9621 - val_loss: 0.1173 - val_acc: 0.9540\n",
            "\n",
            "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 51/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9660Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0859 - acc: 0.9661 - val_loss: 0.1132 - val_acc: 0.9576\n",
            "\n",
            "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 52/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9657Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0838 - acc: 0.9657 - val_loss: 0.1174 - val_acc: 0.9561\n",
            "\n",
            "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 53/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9659Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0835 - acc: 0.9660 - val_loss: 0.1033 - val_acc: 0.9642\n",
            "\n",
            "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 54/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9659Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0847 - acc: 0.9659 - val_loss: 0.1024 - val_acc: 0.9619\n",
            "\n",
            "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 55/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9675Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0816 - acc: 0.9675 - val_loss: 0.1141 - val_acc: 0.9605\n",
            "\n",
            "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 56/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9671Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0786 - acc: 0.9672 - val_loss: 0.1286 - val_acc: 0.9539\n",
            "\n",
            "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 57/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9701Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0757 - acc: 0.9702 - val_loss: 0.1068 - val_acc: 0.9637\n",
            "\n",
            "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 58/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9680Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0806 - acc: 0.9680 - val_loss: 0.1253 - val_acc: 0.9556\n",
            "\n",
            "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 59/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9676Epoch 1/150\n",
            "470/470 [==============================] - 232s 495ms/step - loss: 0.0824 - acc: 0.9677 - val_loss: 0.1121 - val_acc: 0.9624\n",
            "\n",
            "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 60/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9710Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0756 - acc: 0.9710 - val_loss: 0.1162 - val_acc: 0.9573\n",
            "\n",
            "Epoch 00061: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 61/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9740Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0667 - acc: 0.9740 - val_loss: 0.1142 - val_acc: 0.9611\n",
            "\n",
            "Epoch 00062: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 62/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9745Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0645 - acc: 0.9745 - val_loss: 0.1067 - val_acc: 0.9637\n",
            "\n",
            "Epoch 00063: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 63/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9729Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0688 - acc: 0.9729 - val_loss: 0.1018 - val_acc: 0.9666\n",
            "\n",
            "Epoch 00064: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 64/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9728Epoch 1/150\n",
            "470/470 [==============================] - 232s 493ms/step - loss: 0.0683 - acc: 0.9728 - val_loss: 0.1210 - val_acc: 0.9579\n",
            "\n",
            "Epoch 00065: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 65/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9741Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0671 - acc: 0.9741 - val_loss: 0.1099 - val_acc: 0.9624\n",
            "\n",
            "Epoch 00066: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 66/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9725Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0678 - acc: 0.9726 - val_loss: 0.1082 - val_acc: 0.9616\n",
            "\n",
            "Epoch 00067: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 67/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9744Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0650 - acc: 0.9744 - val_loss: 0.1070 - val_acc: 0.9634\n",
            "\n",
            "Epoch 00068: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 68/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9753Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0634 - acc: 0.9753 - val_loss: 0.1041 - val_acc: 0.9663\n",
            "\n",
            "Epoch 00069: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 69/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9751Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0640 - acc: 0.9751 - val_loss: 0.1357 - val_acc: 0.9558\n",
            "\n",
            "Epoch 00070: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 70/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9749Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0636 - acc: 0.9749 - val_loss: 0.1022 - val_acc: 0.9647\n",
            "\n",
            "Epoch 00071: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 71/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9772Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0583 - acc: 0.9773 - val_loss: 0.1116 - val_acc: 0.9626\n",
            "\n",
            "Epoch 00072: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 72/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9785Epoch 1/150\n",
            "470/470 [==============================] - 232s 495ms/step - loss: 0.0556 - acc: 0.9785 - val_loss: 0.1288 - val_acc: 0.9590\n",
            "\n",
            "Epoch 00073: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 73/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9781Epoch 1/150\n",
            "470/470 [==============================] - 233s 496ms/step - loss: 0.0559 - acc: 0.9781 - val_loss: 0.1072 - val_acc: 0.9648\n",
            "\n",
            "Epoch 00074: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 74/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9774Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0574 - acc: 0.9773 - val_loss: 0.1123 - val_acc: 0.9624\n",
            "\n",
            "Epoch 00075: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 75/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9781Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0561 - acc: 0.9782 - val_loss: 0.1202 - val_acc: 0.9627\n",
            "\n",
            "Epoch 00076: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 76/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9797Epoch 1/150\n",
            "470/470 [==============================] - 232s 494ms/step - loss: 0.0544 - acc: 0.9797 - val_loss: 0.1179 - val_acc: 0.9627\n",
            "\n",
            "Epoch 00077: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 77/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9776Epoch 1/150\n",
            "155/470 [========>.....................] - ETA: 34s - loss: 0.1118 - acc: 0.9640Restoring model weights from the end of the best epoch.\n",
            "470/470 [==============================] - 233s 495ms/step - loss: 0.0563 - acc: 0.9775 - val_loss: 0.1118 - val_acc: 0.9640\n",
            "Epoch 00077: early stopping\n",
            "Planned epochs: 150 Calculated epochs : 77 Time elapsed: 4:58:33.383582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRQtvuF2AF21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_filenames = os.listdir(\"../input/test1/test1\")\n",
        "test_df = pd.DataFrame({\n",
        "    'filename': test_filenames\n",
        "})\n",
        "nb_samples = test_df.shape[0]\n",
        "test_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_gen.flow_from_dataframe(\n",
        "    test_df, \n",
        "    \"../input/test1/test1/\", \n",
        "    x_col='filename',\n",
        "    y_col=None,\n",
        "    class_mode=None,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "predict = model.predict_generator(test_generator, steps=np.ceil(nb_samples/batch_size))\n",
        "test_df['category'] = np.argmax(predict, axis=-1)\n",
        "label_map = dict((v,k) for k,v in train_generator.class_indices.items())\n",
        "test_df['category'] = test_df['category'].replace(label_map)\n",
        "test_df['category'] = test_df['category'].replace({ 'dog': 1, 'cat': 0 })\n",
        "\n",
        "\n",
        "sample_test = test_df.head(18)\n",
        "sample_test.head()\n",
        "plt.figure(figsize=(12, 24))\n",
        "for index, row in sample_test.iterrows():\n",
        "    filename = row['filename']\n",
        "    category = row['category']\n",
        "    img = load_img(\"../input/test1/test1/\"+filename, target_size=IMAGE_SIZE)\n",
        "    plt.subplot(6, 3, index+1)\n",
        "    plt.imshow(img)\n",
        "    plt.xlabel(filename + '(' + \"{}\".format(category) + ')' )\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKuWZeFY5Nq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir='log_dir'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7li9n2D0HO3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/fit "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4Qb_0xevhSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(15,12))\n",
        "gs = fig.add_gridspec(1, 1)\n",
        "ax1 = fig.add_subplot()\n",
        "# Don't allow the axis to be on top of your data\n",
        "ax1.set_axisbelow(True)\n",
        "# Turn on the minor TICKS, which are required for the minor GRID\n",
        "ax1.minorticks_on()\n",
        "# Customize the major grid\n",
        "ax1.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
        "# Customize the minor grid\n",
        "ax1.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\n",
        "N = np.arange(0, len(history.history[\"acc\"]) )\n",
        "plt.plot(N, history.history[\"loss\"], linestyle='-', color='red', label=\"Training loss\")\n",
        "plt.plot(N, history.history[\"acc\"], linestyle='-.', color='magenta', label=\"Training accuracy\")\n",
        "plt.plot(N, history.history[\"val_loss\"], linestyle='--', color='blue', label=\"Validation loss\")\n",
        "plt.plot(N, history.history[\"val_acc\"], linestyle=':', color='violet', label=\"Validation accuracy\")\n",
        "plt.title(\"Training/validation Loss and Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(os.path.join(home_dir,'figures', ('fig_' + model_name + str(datetime.datetime.now(timezone).strftime(\"%d-%m-%Y_%H-%M\"))+'.png')))\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP4OUNWCUuhA",
        "colab_type": "code",
        "outputId": "97e54ad3-a132-4208-85b0-86df87d69d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "tensorboard --inspect --logdir='log_dir'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-979fc7d795e1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tensorboard --inspect --logdir='log_dir'\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
          ]
        }
      ]
    }
  ]
}