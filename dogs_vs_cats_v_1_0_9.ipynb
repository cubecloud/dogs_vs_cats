{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dogs_vs_cats-v-1-0-9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cubecloud/dogs_vs_cats/blob/master/dogs_vs_cats_v_1_0_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAl-JTvzvbHC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qJfJPd0PrzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "%tensorflow_version 1.15.2\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NJL_gODxWhp",
        "colab_type": "code",
        "outputId": "e5d77c3a-5d8d-45fe-9625-e17629dfb6d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "%tensorflow_version 1.15.2\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.15.2`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TTUHDm3P7oZ",
        "colab_type": "code",
        "outputId": "8a32bc91-e507-4578-ab5b-8def91fa65b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D-uEuguvcgl",
        "colab_type": "code",
        "outputId": "30af69bb-298d-4031-fe78-a58bce4e8330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "%tensorflow_version 1.15.2\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "import os\n",
        "import glob\n",
        "import pytz\n",
        "import datetime\n",
        "from math import exp\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.15.2`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR_nxwysvW3p",
        "colab_type": "code",
        "outputId": "ba3566d3-d6fb-4e9d-b50a-eaed11537328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "version=109\n",
        "timezone = pytz.timezone(\"Europe/Moscow\")\n",
        "image_size = 150\n",
        "batch_size = 40\n",
        "epochs = 150\n",
        "start_learning_rate = 0.001\n",
        "start_patience = round(epochs*0.04)\n",
        "\n",
        "print (f'Image Size = {image_size}x{image_size}')\n",
        "\n",
        "def make_model(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    # Image augmentation block\n",
        "    # x = data_augmentation(inputs)\n",
        "    x = inputs\n",
        "    # Entry block\n",
        "    # x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(x)\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization( momentum=0.9 )(x) # added momentum [ 0.9 ]\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    x = layers.Dropout(0.5)(x) #<= added dropout layer [ ]\n",
        "\n",
        "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization( momentum=0.9 )(x) # added momentum [ 0.9 ]\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    for size in [128, 256, 512, 728]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization( momentum=0.9)(x) # added momentum [ 0.9 ]\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        \n",
        "        # x = layers.Dropout(0.5)(x) #<= added dropout layer [ ]\n",
        "        \n",
        "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization( momentum=0.9 )(x) # added momentum [ 0.9 ]\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization( momentum=0.9 )(x) # added momentum [ 0.9 ]\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    x = layers.Dropout(0.5)(x) #<= added dropout layer [ 0.2 0.5 ]\n",
        "    \n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    if num_classes == 2:\n",
        "        activation = \"sigmoid\"\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = \"softmax\"\n",
        "        units = num_classes\n",
        "\n",
        "    x = layers.Dropout(0.6)(x) #<= dropout [ 0.5, 0.52, 0.5, 0.6 ,0.6 ]\n",
        "    outputs = layers.Dense(units, activation=activation)(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "home_dir = \"/content/drive/My Drive/Colab Notebooks/dogs_vs_cats/\"\n",
        "base_dir = \"/content/drive/My Drive/Colab Notebooks/dogs_vs_cats/data/\"\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# data loading\n",
        "# ----------------------------------------------------\n",
        "\n",
        "x_train = np.load(os.path.join(home_dir, 'dogs_vs_cats_photos_train.npy'))\n",
        "y_train = np.load(os.path.join(home_dir,'dogs_vs_cats_labels_train.npy'))[2:,]\n",
        "x_validation = np.load(os.path.join(home_dir,'dogs_vs_cats_photos_validation.npy'))\n",
        "y_validation = np.load(os.path.join(home_dir,'dogs_vs_cats_labels_validation.npy'))[2:,]\n",
        "print (x_train.shape, y_train.shape)\n",
        "print (x_validation.shape, y_validation.shape)\n",
        "# ----------------------------------------------------\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30, # <= [ 40, 30 ]\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False,\n",
        "    fill_mode=\"nearest\")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_datagen.fit(x_train)\n",
        "\n",
        "train_generator = train_datagen.flow(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "validation_generator = validation_datagen.flow(\n",
        "    x=x_validation,\n",
        "    y=y_validation,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "model = make_model(input_shape=(image_size,image_size) + (3,), num_classes=2)\n",
        "keras.utils.plot_model(model, show_shapes=True)\n",
        "model.summary()\n",
        "\n",
        "def scheduler(epoch):\n",
        "  if epoch < 10:\n",
        "    return start_learning_rate\n",
        "  else:\n",
        "    # return start_learning_rate * exp(0.1 * (10 - epoch))\n",
        "    return start_learning_rate*(0.75**(epoch//10))\n",
        "\n",
        "class EarlyStoppingAtMinValLoss(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, start_learning_rate=start_learning_rate, patience=3):\n",
        "    super(EarlyStoppingAtMinValLoss, self).__init__()\n",
        "    self.patience = patience\n",
        "    self.start_learning_rate = start_learning_rate\n",
        "    # best_weights to store the weights at which the minimum loss occurs.\n",
        "    self.best_weights = None\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    # The number of epoch it has waited when loss is no longer minimum.\n",
        "    self.wait = 0\n",
        "    # The epoch the training stops at.\n",
        "    self.stopped_epoch = 0\n",
        "    # Initialize the best as infinity.\n",
        "    self.best = np.Inf\n",
        "  \n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    if (epoch >= 10) and (epoch%10 == 0):\n",
        "      self.patience += round((epochs-epoch)*0.03)\n",
        "    # self.patience = round((self.start_learning_rate / scheduler (epoch) + (epochs*0.15))/2)\n",
        "    print (f'Patience: {self.patience}')\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    current = logs.get('val_loss')\n",
        "    if np.less(current, self.best):\n",
        "      self.best = current\n",
        "      self.wait = 0\n",
        "      # Record the best weights if current results is better (less).\n",
        "      self.best_weights = self.model.get_weights()\n",
        "    else:\n",
        "      self.wait += 1\n",
        "      if self.wait >= self.patience:\n",
        "        self.stopped_epoch = epoch\n",
        "        self.model.stop_training = True\n",
        "        print('Restoring model weights from the end of the best epoch.')\n",
        "        self.model.set_weights(self.best_weights)\n",
        "\n",
        "  def on_train_end(self, logs=None):\n",
        "    if self.stopped_epoch > 0:\n",
        "      print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
        "\n",
        "  # def on_train_batch_begin(self, batch, logs=None):\n",
        "  #   print('Training: batch {} begins at {}'.format(batch, datetime.datetime.now(timezone).time()))\n",
        "\n",
        "\n",
        "lrs = keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
        "log_dir = os.path.join(home_dir, 'logs/fit')\n",
        "esmvl = EarlyStoppingAtMinValLoss (patience=start_patience)\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=start_patience, verbose=1)\n",
        "tb = TensorBoard(log_dir=log_dir, histogram_freq=10, write_graph=True, write_images=False )\n",
        "\n",
        "def filename_with_time (prefix_str, root_str):\n",
        "  return prefix_str + root_str + str(datetime.datetime.now(timezone).strftime(\"%d-%m-%Y_%H-%M\"))\n",
        "\n",
        "model_name= f'cats_and_dogs_{str(version)}_{str(image_size)}x{str(image_size)}_{str(batch_size)}_'\n",
        "chkp = keras.callbacks.ModelCheckpoint(os.path.join(home_dir,'save/', filename_with_time(model_name,\"save_at_{epoch:02d}-{val_loss:.4f}_\") + \".h5\"), monitor='val_loss', save_best_only=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(lr=start_learning_rate),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "callbacks = [lrs,  esmvl, chkp ]\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = len(train_generator),\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = len(validation_generator),\n",
        "    epochs = epochs,\n",
        "    verbose=1,\n",
        "    callbacks = callbacks)\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print (f'Planned epochs: {epochs} Calculated epochs : {len(history.history[\"loss\"])} Time elapsed: {end - start}')\n",
        "epochs = len(history.history['loss'])\n",
        "model_name= f'cats_and_dogs_{str(version)}_{str(image_size)}x{str(image_size)}_{str(batch_size)}_epochs-{epochs}_'\n",
        "model.save(os.path.join(home_dir, model_name + str(datetime.datetime.now(timezone).strftime(\"%d-%m-%Y_%H-%M\")) + '.h5'))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(15,12))\n",
        "gs = fig.add_gridspec(1, 1)\n",
        "ax1 = fig.add_subplot()\n",
        "# Don't allow the axis to be on top of your data\n",
        "ax1.set_axisbelow(True)\n",
        "# Turn on the minor TICKS, which are required for the minor GRID\n",
        "ax1.minorticks_on()\n",
        "# Customize the major grid\n",
        "ax1.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
        "# Customize the minor grid\n",
        "ax1.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\n",
        "N = np.arange(0, len(history.history[\"acc\"]) )\n",
        "plt.plot(N, history.history[\"loss\"], linestyle='-', color='red', label=\"Training loss\")\n",
        "plt.plot(N, history.history[\"acc\"], linestyle='-.', color='magenta', label=\"Training accuracy\")\n",
        "plt.plot(N, history.history[\"val_loss\"], linestyle='--', color='blue', label=\"Validation loss\")\n",
        "plt.plot(N, history.history[\"val_acc\"], linestyle=':', color='violet', label=\"Validation accuracy\")\n",
        "plt.title(\"Training/validation Loss and Accuracy\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(home_dir,'figures', ('fig_' + model_name + str(datetime.datetime.now(timezone).strftime(\"%d-%m-%Y_%H-%M\"))+'.png')))\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image Size = 150x150\n",
            "(18800, 150, 150, 3) (18800,)\n",
            "(6200, 150, 150, 3) (6200,)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 150, 150, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 75, 75, 32)   896         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 75, 75, 32)   128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 75, 75, 32)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 75, 75, 32)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 75, 75, 64)   18496       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 75, 75, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 75, 75, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 75, 75, 64)   0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d (SeparableConv (None, 75, 75, 128)  8896        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 75, 75, 128)  512         separable_conv2d[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 75, 75, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 75, 75, 128)  17664       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 75, 75, 128)  512         separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 38, 38, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 38, 38, 128)  8320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 38, 38, 128)  0           max_pooling2d[0][0]              \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 38, 38, 128)  0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 38, 38, 256)  34176       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 38, 38, 256)  1024        separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 38, 38, 256)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_3 (SeparableCo (None, 38, 38, 256)  68096       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 38, 38, 256)  1024        separable_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 19, 19, 256)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 19, 19, 256)  33024       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 19, 19, 256)  0           max_pooling2d_1[0][0]            \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 19, 19, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_4 (SeparableCo (None, 19, 19, 512)  133888      activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 19, 19, 512)  2048        separable_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 19, 19, 512)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_5 (SeparableCo (None, 19, 19, 512)  267264      activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 19, 19, 512)  2048        separable_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 512)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 10, 10, 512)  131584      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 10, 10, 512)  0           max_pooling2d_2[0][0]            \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 10, 10, 512)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_6 (SeparableCo (None, 10, 10, 728)  378072      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 10, 10, 728)  2912        separable_conv2d_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 10, 10, 728)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_7 (SeparableCo (None, 10, 10, 728)  537264      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 10, 10, 728)  2912        separable_conv2d_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 728)    0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 5, 5, 728)    373464      add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 5, 5, 728)    0           max_pooling2d_3[0][0]            \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_8 (SeparableCo (None, 5, 5, 1024)   753048      add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 5, 5, 1024)   4096        separable_conv2d_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 5, 5, 1024)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 5, 5, 1024)   0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 1024)         0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 1024)         0           global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            1025        dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,782,649\n",
            "Trainable params: 2,773,913\n",
            "Non-trainable params: 8,736\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 1/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.6905 - acc: 0.6039Epoch 1/150\n",
            "470/470 [==============================] - 138s 293ms/step - loss: 0.6904 - acc: 0.6039 - val_loss: 0.6367 - val_acc: 0.6627\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 2/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.5850 - acc: 0.6991Epoch 1/150\n",
            "470/470 [==============================] - 122s 260ms/step - loss: 0.5850 - acc: 0.6991 - val_loss: 0.5450 - val_acc: 0.7356\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 3/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.5015 - acc: 0.7569Epoch 1/150\n",
            "470/470 [==============================] - 121s 258ms/step - loss: 0.5015 - acc: 0.7569 - val_loss: 0.4128 - val_acc: 0.8134\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 4/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.7878Epoch 1/150\n",
            "470/470 [==============================] - 121s 257ms/step - loss: 0.4530 - acc: 0.7879 - val_loss: 0.3575 - val_acc: 0.8455\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 5/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8209Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.3954 - acc: 0.8209 - val_loss: 0.3299 - val_acc: 0.8584\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 6/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8432Epoch 1/150\n",
            "470/470 [==============================] - 121s 258ms/step - loss: 0.3506 - acc: 0.8433 - val_loss: 0.4665 - val_acc: 0.8158\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 7/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.3199 - acc: 0.8582Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.3199 - acc: 0.8582 - val_loss: 0.6368 - val_acc: 0.7887\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 8/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2900 - acc: 0.8752Epoch 1/150\n",
            "470/470 [==============================] - 121s 257ms/step - loss: 0.2900 - acc: 0.8753 - val_loss: 0.2105 - val_acc: 0.9129\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 9/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2669 - acc: 0.8857Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.2667 - acc: 0.8858 - val_loss: 0.1774 - val_acc: 0.9318\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Patience: 6\n",
            "Epoch 10/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2624 - acc: 0.8875Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.2622 - acc: 0.8876 - val_loss: 0.1688 - val_acc: 0.9342\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 11/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9019Epoch 1/150\n",
            "470/470 [==============================] - 121s 258ms/step - loss: 0.2299 - acc: 0.9021 - val_loss: 0.1676 - val_acc: 0.9311\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 12/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9063Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.2178 - acc: 0.9064 - val_loss: 0.1491 - val_acc: 0.9374\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 13/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9107Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.2124 - acc: 0.9108 - val_loss: 0.1750 - val_acc: 0.9331\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 14/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9171Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.2004 - acc: 0.9172 - val_loss: 0.2563 - val_acc: 0.9027\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 15/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9168Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.1931 - acc: 0.9166 - val_loss: 0.1416 - val_acc: 0.9439\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 16/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9205Epoch 1/150\n",
            "470/470 [==============================] - 121s 257ms/step - loss: 0.1873 - acc: 0.9206 - val_loss: 0.1610 - val_acc: 0.9355\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 17/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9251Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.1806 - acc: 0.9252 - val_loss: 0.1801 - val_acc: 0.9265\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 18/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9248Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.1787 - acc: 0.9248 - val_loss: 0.1365 - val_acc: 0.9484\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 19/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9284Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.1735 - acc: 0.9285 - val_loss: 0.1225 - val_acc: 0.9519\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00075.\n",
            "Patience: 10\n",
            "Epoch 20/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9302Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.1702 - acc: 0.9302 - val_loss: 0.1199 - val_acc: 0.9532\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 21/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9377Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.1547 - acc: 0.9374 - val_loss: 0.1203 - val_acc: 0.9544\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 22/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9391Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.1480 - acc: 0.9391 - val_loss: 0.1081 - val_acc: 0.9556\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 23/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9415Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.1424 - acc: 0.9414 - val_loss: 0.1329 - val_acc: 0.9523\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 24/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9421Epoch 1/150\n",
            "470/470 [==============================] - 119s 254ms/step - loss: 0.1416 - acc: 0.9419 - val_loss: 0.1275 - val_acc: 0.9544\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 25/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9439Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.1368 - acc: 0.9439 - val_loss: 0.1370 - val_acc: 0.9506\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 26/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9445Epoch 1/150\n",
            "470/470 [==============================] - 119s 252ms/step - loss: 0.1349 - acc: 0.9445 - val_loss: 0.1118 - val_acc: 0.9547\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 27/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1322 - acc: 0.9450Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.1323 - acc: 0.9450 - val_loss: 0.1147 - val_acc: 0.9558\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 28/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9454Epoch 1/150\n",
            "470/470 [==============================] - 119s 254ms/step - loss: 0.1306 - acc: 0.9453 - val_loss: 0.1015 - val_acc: 0.9597\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 29/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9478Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.1284 - acc: 0.9476 - val_loss: 0.1125 - val_acc: 0.9613\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
            "Patience: 14\n",
            "Epoch 30/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9488Epoch 1/150\n",
            "470/470 [==============================] - 119s 254ms/step - loss: 0.1264 - acc: 0.9488 - val_loss: 0.1044 - val_acc: 0.9569\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 31/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9549Epoch 1/150\n",
            "470/470 [==============================] - 119s 254ms/step - loss: 0.1153 - acc: 0.9548 - val_loss: 0.1073 - val_acc: 0.9592\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 32/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9558Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.1120 - acc: 0.9557 - val_loss: 0.1023 - val_acc: 0.9619\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 33/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9571Epoch 1/150\n",
            "470/470 [==============================] - 119s 254ms/step - loss: 0.1058 - acc: 0.9572 - val_loss: 0.1416 - val_acc: 0.9503\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 34/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9587Epoch 1/150\n",
            "470/470 [==============================] - 120s 254ms/step - loss: 0.1030 - acc: 0.9588 - val_loss: 0.1273 - val_acc: 0.9552\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 35/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9579Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.1060 - acc: 0.9580 - val_loss: 0.0978 - val_acc: 0.9639\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 36/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9588Epoch 1/150\n",
            "470/470 [==============================] - 121s 258ms/step - loss: 0.1022 - acc: 0.9589 - val_loss: 0.0973 - val_acc: 0.9656\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 37/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9599Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.1035 - acc: 0.9599 - val_loss: 0.1373 - val_acc: 0.9471\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 38/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9595Epoch 1/150\n",
            "470/470 [==============================] - 120s 254ms/step - loss: 0.1006 - acc: 0.9595 - val_loss: 0.1344 - val_acc: 0.9500\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 39/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9608Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0969 - acc: 0.9607 - val_loss: 0.1117 - val_acc: 0.9587\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 0.000421875.\n",
            "Patience: 18\n",
            "Epoch 40/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9606Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0970 - acc: 0.9606 - val_loss: 0.0967 - val_acc: 0.9666\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 41/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9641Epoch 1/150\n",
            "470/470 [==============================] - 119s 254ms/step - loss: 0.0897 - acc: 0.9641 - val_loss: 0.0978 - val_acc: 0.9629\n",
            "\n",
            "Epoch 00042: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 42/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9651Epoch 1/150\n",
            "470/470 [==============================] - 119s 254ms/step - loss: 0.0866 - acc: 0.9651 - val_loss: 0.1190 - val_acc: 0.9547\n",
            "\n",
            "Epoch 00043: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 43/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9648Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0879 - acc: 0.9648 - val_loss: 0.1025 - val_acc: 0.9621\n",
            "\n",
            "Epoch 00044: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 44/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9650Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.0831 - acc: 0.9650 - val_loss: 0.1052 - val_acc: 0.9645\n",
            "\n",
            "Epoch 00045: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 45/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9654Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0879 - acc: 0.9654 - val_loss: 0.1012 - val_acc: 0.9660\n",
            "\n",
            "Epoch 00046: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 46/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9675Epoch 1/150\n",
            "470/470 [==============================] - 121s 258ms/step - loss: 0.0793 - acc: 0.9675 - val_loss: 0.0970 - val_acc: 0.9660\n",
            "\n",
            "Epoch 00047: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 47/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9660Epoch 1/150\n",
            "470/470 [==============================] - 121s 257ms/step - loss: 0.0853 - acc: 0.9660 - val_loss: 0.0974 - val_acc: 0.9647\n",
            "\n",
            "Epoch 00048: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 48/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9696Epoch 1/150\n",
            "470/470 [==============================] - 121s 258ms/step - loss: 0.0773 - acc: 0.9696 - val_loss: 0.1039 - val_acc: 0.9645\n",
            "\n",
            "Epoch 00049: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 49/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9688Epoch 1/150\n",
            "470/470 [==============================] - 121s 258ms/step - loss: 0.0791 - acc: 0.9688 - val_loss: 0.0900 - val_acc: 0.9653\n",
            "\n",
            "Epoch 00050: LearningRateScheduler reducing learning rate to 0.00031640625.\n",
            "Patience: 21\n",
            "Epoch 50/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9703Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0760 - acc: 0.9703 - val_loss: 0.1038 - val_acc: 0.9621\n",
            "\n",
            "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 51/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9719Epoch 1/150\n",
            "470/470 [==============================] - 119s 254ms/step - loss: 0.0706 - acc: 0.9718 - val_loss: 0.0962 - val_acc: 0.9673\n",
            "\n",
            "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 52/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9755Epoch 1/150\n",
            "470/470 [==============================] - 120s 254ms/step - loss: 0.0634 - acc: 0.9754 - val_loss: 0.0984 - val_acc: 0.9689\n",
            "\n",
            "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 53/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9740Epoch 1/150\n",
            "470/470 [==============================] - 118s 252ms/step - loss: 0.0649 - acc: 0.9739 - val_loss: 0.1339 - val_acc: 0.9585\n",
            "\n",
            "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 54/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9764Epoch 1/150\n",
            "470/470 [==============================] - 118s 252ms/step - loss: 0.0605 - acc: 0.9765 - val_loss: 0.0896 - val_acc: 0.9690\n",
            "\n",
            "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 55/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9727Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0653 - acc: 0.9728 - val_loss: 0.0926 - val_acc: 0.9684\n",
            "\n",
            "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 56/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9746Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.0638 - acc: 0.9746 - val_loss: 0.0936 - val_acc: 0.9684\n",
            "\n",
            "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 57/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9756Epoch 1/150\n",
            "470/470 [==============================] - 121s 258ms/step - loss: 0.0622 - acc: 0.9756 - val_loss: 0.0991 - val_acc: 0.9634\n",
            "\n",
            "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 58/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9749Epoch 1/150\n",
            "470/470 [==============================] - 124s 264ms/step - loss: 0.0618 - acc: 0.9749 - val_loss: 0.1080 - val_acc: 0.9644\n",
            "\n",
            "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 59/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9738Epoch 1/150\n",
            "470/470 [==============================] - 124s 265ms/step - loss: 0.0654 - acc: 0.9738 - val_loss: 0.0924 - val_acc: 0.9682\n",
            "\n",
            "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0002373046875.\n",
            "Patience: 24\n",
            "Epoch 60/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9761Epoch 1/150\n",
            "470/470 [==============================] - 125s 266ms/step - loss: 0.0610 - acc: 0.9760 - val_loss: 0.0955 - val_acc: 0.9669\n",
            "\n",
            "Epoch 00061: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 61/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9772Epoch 1/150\n",
            "470/470 [==============================] - 126s 268ms/step - loss: 0.0583 - acc: 0.9771 - val_loss: 0.1023 - val_acc: 0.9650\n",
            "\n",
            "Epoch 00062: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 62/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9802Epoch 1/150\n",
            "470/470 [==============================] - 124s 264ms/step - loss: 0.0518 - acc: 0.9802 - val_loss: 0.1023 - val_acc: 0.9655\n",
            "\n",
            "Epoch 00063: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 63/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9797Epoch 1/150\n",
            "470/470 [==============================] - 122s 259ms/step - loss: 0.0523 - acc: 0.9797 - val_loss: 0.0962 - val_acc: 0.9697\n",
            "\n",
            "Epoch 00064: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 64/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9795Epoch 1/150\n",
            "470/470 [==============================] - 121s 258ms/step - loss: 0.0541 - acc: 0.9795 - val_loss: 0.0933 - val_acc: 0.9697\n",
            "\n",
            "Epoch 00065: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 65/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9806Epoch 1/150\n",
            "470/470 [==============================] - 121s 257ms/step - loss: 0.0541 - acc: 0.9806 - val_loss: 0.0901 - val_acc: 0.9700\n",
            "\n",
            "Epoch 00066: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 66/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9795Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0521 - acc: 0.9795 - val_loss: 0.0906 - val_acc: 0.9684\n",
            "\n",
            "Epoch 00067: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 67/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9781Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0554 - acc: 0.9781 - val_loss: 0.1215 - val_acc: 0.9598\n",
            "\n",
            "Epoch 00068: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 68/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9808Epoch 1/150\n",
            "470/470 [==============================] - 121s 257ms/step - loss: 0.0497 - acc: 0.9808 - val_loss: 0.0883 - val_acc: 0.9702\n",
            "\n",
            "Epoch 00069: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 69/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9809Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.0496 - acc: 0.9810 - val_loss: 0.0917 - val_acc: 0.9708\n",
            "\n",
            "Epoch 00070: LearningRateScheduler reducing learning rate to 0.000177978515625.\n",
            "Patience: 27\n",
            "Epoch 70/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9816Epoch 1/150\n",
            "470/470 [==============================] - 120s 254ms/step - loss: 0.0472 - acc: 0.9816 - val_loss: 0.0947 - val_acc: 0.9706\n",
            "\n",
            "Epoch 00071: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 71/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9821Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.0467 - acc: 0.9821 - val_loss: 0.1060 - val_acc: 0.9674\n",
            "\n",
            "Epoch 00072: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 72/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9809Epoch 1/150\n",
            "470/470 [==============================] - 119s 254ms/step - loss: 0.0464 - acc: 0.9810 - val_loss: 0.0961 - val_acc: 0.9706\n",
            "\n",
            "Epoch 00073: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 73/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9825Epoch 1/150\n",
            "470/470 [==============================] - 119s 254ms/step - loss: 0.0440 - acc: 0.9826 - val_loss: 0.0923 - val_acc: 0.9706\n",
            "\n",
            "Epoch 00074: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 74/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9832Epoch 1/150\n",
            "470/470 [==============================] - 119s 254ms/step - loss: 0.0428 - acc: 0.9832 - val_loss: 0.1074 - val_acc: 0.9665\n",
            "\n",
            "Epoch 00075: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 75/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9830Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.0463 - acc: 0.9830 - val_loss: 0.0917 - val_acc: 0.9713\n",
            "\n",
            "Epoch 00076: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 76/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9845Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0423 - acc: 0.9844 - val_loss: 0.0953 - val_acc: 0.9705\n",
            "\n",
            "Epoch 00077: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 77/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9839Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0408 - acc: 0.9839 - val_loss: 0.1105 - val_acc: 0.9673\n",
            "\n",
            "Epoch 00078: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 78/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9838Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0416 - acc: 0.9838 - val_loss: 0.1037 - val_acc: 0.9681\n",
            "\n",
            "Epoch 00079: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 79/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9834Epoch 1/150\n",
            "470/470 [==============================] - 119s 254ms/step - loss: 0.0418 - acc: 0.9834 - val_loss: 0.1088 - val_acc: 0.9685\n",
            "\n",
            "Epoch 00080: LearningRateScheduler reducing learning rate to 0.00013348388671875.\n",
            "Patience: 29\n",
            "Epoch 80/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9827Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.0436 - acc: 0.9827 - val_loss: 0.0967 - val_acc: 0.9692\n",
            "\n",
            "Epoch 00081: LearningRateScheduler reducing learning rate to 0.0001001129150390625.\n",
            "Patience: 31\n",
            "Epoch 81/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9859Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.0396 - acc: 0.9860 - val_loss: 0.0962 - val_acc: 0.9690\n",
            "\n",
            "Epoch 00082: LearningRateScheduler reducing learning rate to 0.0001001129150390625.\n",
            "Patience: 31\n",
            "Epoch 82/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9865Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.0350 - acc: 0.9865 - val_loss: 0.1117 - val_acc: 0.9668\n",
            "\n",
            "Epoch 00083: LearningRateScheduler reducing learning rate to 0.0001001129150390625.\n",
            "Patience: 31\n",
            "Epoch 83/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9847Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.0417 - acc: 0.9847 - val_loss: 0.0963 - val_acc: 0.9702\n",
            "\n",
            "Epoch 00084: LearningRateScheduler reducing learning rate to 0.0001001129150390625.\n",
            "Patience: 31\n",
            "Epoch 84/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9857Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.0368 - acc: 0.9857 - val_loss: 0.0988 - val_acc: 0.9710\n",
            "\n",
            "Epoch 00085: LearningRateScheduler reducing learning rate to 0.0001001129150390625.\n",
            "Patience: 31\n",
            "Epoch 85/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9868Epoch 1/150\n",
            "470/470 [==============================] - 119s 252ms/step - loss: 0.0352 - acc: 0.9868 - val_loss: 0.1159 - val_acc: 0.9663\n",
            "\n",
            "Epoch 00086: LearningRateScheduler reducing learning rate to 0.0001001129150390625.\n",
            "Patience: 31\n",
            "Epoch 86/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9857Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.0360 - acc: 0.9856 - val_loss: 0.0984 - val_acc: 0.9700\n",
            "\n",
            "Epoch 00087: LearningRateScheduler reducing learning rate to 0.0001001129150390625.\n",
            "Patience: 31\n",
            "Epoch 87/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9852Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.0371 - acc: 0.9852 - val_loss: 0.0988 - val_acc: 0.9702\n",
            "\n",
            "Epoch 00088: LearningRateScheduler reducing learning rate to 0.0001001129150390625.\n",
            "Patience: 31\n",
            "Epoch 88/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9859Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.0362 - acc: 0.9859 - val_loss: 0.0968 - val_acc: 0.9729\n",
            "\n",
            "Epoch 00089: LearningRateScheduler reducing learning rate to 0.0001001129150390625.\n",
            "Patience: 31\n",
            "Epoch 89/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9872Epoch 1/150\n",
            "470/470 [==============================] - 119s 252ms/step - loss: 0.0351 - acc: 0.9872 - val_loss: 0.0968 - val_acc: 0.9726\n",
            "\n",
            "Epoch 00090: LearningRateScheduler reducing learning rate to 0.0001001129150390625.\n",
            "Patience: 31\n",
            "Epoch 90/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9866Epoch 1/150\n",
            "470/470 [==============================] - 119s 252ms/step - loss: 0.0342 - acc: 0.9866 - val_loss: 0.0969 - val_acc: 0.9711\n",
            "\n",
            "Epoch 00091: LearningRateScheduler reducing learning rate to 7.508468627929687e-05.\n",
            "Patience: 33\n",
            "Epoch 91/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9874Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.0347 - acc: 0.9874 - val_loss: 0.1011 - val_acc: 0.9703\n",
            "\n",
            "Epoch 00092: LearningRateScheduler reducing learning rate to 7.508468627929687e-05.\n",
            "Patience: 33\n",
            "Epoch 92/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9878Epoch 1/150\n",
            "470/470 [==============================] - 123s 262ms/step - loss: 0.0325 - acc: 0.9879 - val_loss: 0.1065 - val_acc: 0.9694\n",
            "\n",
            "Epoch 00093: LearningRateScheduler reducing learning rate to 7.508468627929687e-05.\n",
            "Patience: 33\n",
            "Epoch 93/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9886Epoch 1/150\n",
            "470/470 [==============================] - 122s 259ms/step - loss: 0.0316 - acc: 0.9886 - val_loss: 0.0953 - val_acc: 0.9705\n",
            "\n",
            "Epoch 00094: LearningRateScheduler reducing learning rate to 7.508468627929687e-05.\n",
            "Patience: 33\n",
            "Epoch 94/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9889Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.0315 - acc: 0.9889 - val_loss: 0.1083 - val_acc: 0.9684\n",
            "\n",
            "Epoch 00095: LearningRateScheduler reducing learning rate to 7.508468627929687e-05.\n",
            "Patience: 33\n",
            "Epoch 95/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9870Epoch 1/150\n",
            "470/470 [==============================] - 120s 256ms/step - loss: 0.0323 - acc: 0.9870 - val_loss: 0.1182 - val_acc: 0.9668\n",
            "\n",
            "Epoch 00096: LearningRateScheduler reducing learning rate to 7.508468627929687e-05.\n",
            "Patience: 33\n",
            "Epoch 96/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9882Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.0305 - acc: 0.9881 - val_loss: 0.0988 - val_acc: 0.9710\n",
            "\n",
            "Epoch 00097: LearningRateScheduler reducing learning rate to 7.508468627929687e-05.\n",
            "Patience: 33\n",
            "Epoch 97/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9884Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.0308 - acc: 0.9884 - val_loss: 0.1114 - val_acc: 0.9697\n",
            "\n",
            "Epoch 00098: LearningRateScheduler reducing learning rate to 7.508468627929687e-05.\n",
            "Patience: 33\n",
            "Epoch 98/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9873Epoch 1/150\n",
            "470/470 [==============================] - 119s 253ms/step - loss: 0.0335 - acc: 0.9873 - val_loss: 0.1023 - val_acc: 0.9710\n",
            "\n",
            "Epoch 00099: LearningRateScheduler reducing learning rate to 7.508468627929687e-05.\n",
            "Patience: 33\n",
            "Epoch 99/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9901Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0271 - acc: 0.9901 - val_loss: 0.1126 - val_acc: 0.9700\n",
            "\n",
            "Epoch 00100: LearningRateScheduler reducing learning rate to 7.508468627929687e-05.\n",
            "Patience: 33\n",
            "Epoch 100/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9882Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0325 - acc: 0.9881 - val_loss: 0.0964 - val_acc: 0.9737\n",
            "\n",
            "Epoch 00101: LearningRateScheduler reducing learning rate to 5.6313514709472656e-05.\n",
            "Patience: 35\n",
            "Epoch 101/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9885Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0292 - acc: 0.9886 - val_loss: 0.0957 - val_acc: 0.9718\n",
            "\n",
            "Epoch 00102: LearningRateScheduler reducing learning rate to 5.6313514709472656e-05.\n",
            "Patience: 35\n",
            "Epoch 102/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9889Epoch 1/150\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0303 - acc: 0.9889 - val_loss: 0.0987 - val_acc: 0.9710\n",
            "\n",
            "Epoch 00103: LearningRateScheduler reducing learning rate to 5.6313514709472656e-05.\n",
            "Patience: 35\n",
            "Epoch 103/150\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9907Epoch 1/150\n",
            "155/470 [========>.....................] - ETA: 10s - loss: 0.1081 - acc: 0.9702Restoring model weights from the end of the best epoch.\n",
            "470/470 [==============================] - 120s 255ms/step - loss: 0.0259 - acc: 0.9906 - val_loss: 0.1081 - val_acc: 0.9702\n",
            "Epoch 00103: early stopping\n",
            "Planned epochs: 150 Calculated epochs : 103 Time elapsed: 3:26:42.277185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRQtvuF2AF21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_filenames = os.listdir(\"../input/test1/test1\")\n",
        "test_df = pd.DataFrame({\n",
        "    'filename': test_filenames\n",
        "})\n",
        "nb_samples = test_df.shape[0]\n",
        "test_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_gen.flow_from_dataframe(\n",
        "    test_df, \n",
        "    \"../input/test1/test1/\", \n",
        "    x_col='filename',\n",
        "    y_col=None,\n",
        "    class_mode=None,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "predict = model.predict_generator(test_generator, steps=np.ceil(nb_samples/batch_size))\n",
        "test_df['category'] = np.argmax(predict, axis=-1)\n",
        "label_map = dict((v,k) for k,v in train_generator.class_indices.items())\n",
        "test_df['category'] = test_df['category'].replace(label_map)\n",
        "test_df['category'] = test_df['category'].replace({ 'dog': 1, 'cat': 0 })\n",
        "\n",
        "\n",
        "sample_test = test_df.head(18)\n",
        "sample_test.head()\n",
        "plt.figure(figsize=(12, 24))\n",
        "for index, row in sample_test.iterrows():\n",
        "    filename = row['filename']\n",
        "    category = row['category']\n",
        "    img = load_img(\"../input/test1/test1/\"+filename, target_size=IMAGE_SIZE)\n",
        "    plt.subplot(6, 3, index+1)\n",
        "    plt.imshow(img)\n",
        "    plt.xlabel(filename + '(' + \"{}\".format(category) + ')' )\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKuWZeFY5Nq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir='log_dir'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7li9n2D0HO3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/fit "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4Qb_0xevhSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(15,12))\n",
        "gs = fig.add_gridspec(1, 1)\n",
        "ax1 = fig.add_subplot()\n",
        "# Don't allow the axis to be on top of your data\n",
        "ax1.set_axisbelow(True)\n",
        "# Turn on the minor TICKS, which are required for the minor GRID\n",
        "ax1.minorticks_on()\n",
        "# Customize the major grid\n",
        "ax1.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
        "# Customize the minor grid\n",
        "ax1.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\n",
        "N = np.arange(0, len(history.history[\"acc\"]) )\n",
        "plt.plot(N, history.history[\"loss\"], linestyle='-', color='red', label=\"Training loss\")\n",
        "plt.plot(N, history.history[\"acc\"], linestyle='-.', color='magenta', label=\"Training accuracy\")\n",
        "plt.plot(N, history.history[\"val_loss\"], linestyle='--', color='blue', label=\"Validation loss\")\n",
        "plt.plot(N, history.history[\"val_acc\"], linestyle=':', color='violet', label=\"Validation accuracy\")\n",
        "plt.title(\"Training/validation Loss and Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(os.path.join(home_dir,'figures', ('fig_' + model_name + str(datetime.datetime.now(timezone).strftime(\"%d-%m-%Y_%H-%M\"))+'.png')))\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP4OUNWCUuhA",
        "colab_type": "code",
        "outputId": "97e54ad3-a132-4208-85b0-86df87d69d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "tensorboard --inspect --logdir='log_dir'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-979fc7d795e1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tensorboard --inspect --logdir='log_dir'\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
          ]
        }
      ]
    }
  ]
}